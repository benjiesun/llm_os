#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
llm_vllm.py
è¨€é“ OS â€” è°ƒç”¨æœåŠ¡å™¨ä¸Šè‡ªå»º vLLM æœåŠ¡çš„æ¨¡å—
åŠŸèƒ½ï¼š
- ä»…é€šè¿‡ HTTP æ¥å£ä¸æœåŠ¡å™¨ä¸Šçš„ llm_vllm_server.py äº¤äº’
- æ”¯æŒçŸ­æœŸä¸Šä¸‹æ–‡è®°å¿†ï¼ˆsession-based chatï¼‰
"""

import os
import requests
from dotenv import load_dotenv
from utils.prompt_loader import load_system_prompt

# ========= åŠ è½½ç¯å¢ƒå˜é‡ =========
load_dotenv()
LOCAL_ADDR = os.getenv("LOCAL_ADDR", "http://127.0.0.1:8000/v1")   # é»˜è®¤æœ¬æœºç«¯å£
LOCAL_TIMEOUT = int(os.getenv("LOCAL_TIMEOUT", "60"))

# ========= å…¨å±€ä¼šè¯ç¼“å­˜ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰ =========
# æ ¼å¼: { session_id: [ {"role": "system"/"user"/"assistant", "content": "..."}, ... ] }
CONTEXT_CACHE = {}

# ========= æ ¸å¿ƒå‡½æ•° =========
def get_command_from_llm(prompt: str,
                         system_type: str = None,
                         local_addr: str = None,
                         session_id: str = "default",
                         max_new_tokens: int = 512,
                         temperature: float = 0.7,
                         keep_context: bool = True) -> str:
    """
    è°ƒç”¨æœåŠ¡å™¨ä¸Šçš„ llm_vllm_server.py æœåŠ¡ã€‚
    å‚æ•°ï¼š
        prompt: ç”¨æˆ·è¾“å…¥
        system_type: ç³»ç»Ÿç±»å‹ï¼ˆLinux/Windows/...ï¼‰
        local_addr: API åœ°å€
        session_id: å½“å‰ä¼šè¯æ ‡è¯†ç¬¦
        keep_context: æ˜¯å¦ä¿ç•™ä¸Šä¸‹æ–‡
    è¿”å›ï¼š
        å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
    """
    addr = local_addr or LOCAL_ADDR
    base = addr.rstrip("/")
    url = f"{base}/chat/completions" if not base.endswith("/chat/completions") else base

    # === åˆå§‹åŒ–ä¸Šä¸‹æ–‡ ===
    if session_id not in CONTEXT_CACHE or not keep_context:
        SYSTEM_PROMPT = load_system_prompt(system_type)
        CONTEXT_CACHE[session_id] = [{"role": "system", "content": SYSTEM_PROMPT}]

    # === æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥ ===
    CONTEXT_CACHE[session_id].append({"role": "user", "content": prompt})

    # === å‘é€è¯·æ±‚ ===
    headers = {"Content-Type": "application/json"}
    payload = {
        "model": "local-DeepSeek",
        "messages": CONTEXT_CACHE[session_id],
        "temperature": temperature,
        "max_tokens": max_new_tokens,
        "stream": False
    }

    try:
        response = requests.post(url, headers=headers, json=payload, timeout=LOCAL_TIMEOUT)
        response.raise_for_status()
        data = response.json()

        if "choices" in data and len(data["choices"]) > 0:
            choice = data["choices"][0]
            if "message" in choice and "content" in choice["message"]:
                reply = choice["message"]["content"].strip()
            elif "text" in choice:
                reply = choice["text"].strip()
            else:
                reply = str(data)
        else:
            reply = str(data)

        # === å­˜å…¥å¯¹è¯ä¸Šä¸‹æ–‡ ===
        CONTEXT_CACHE[session_id].append({"role": "assistant", "content": reply})

        return reply

    except Exception as e:
        return f"âŒ æœ¬åœ° vLLM API è¯·æ±‚å¤±è´¥: {e}"


# ========= è¾…åŠ©å‡½æ•° =========
def clear_context(session_id: str = "default"):
    """æ¸…é™¤æŒ‡å®šä¼šè¯çš„ä¸Šä¸‹æ–‡"""
    if session_id in CONTEXT_CACHE:
        del CONTEXT_CACHE[session_id]


# ========= æµ‹è¯•è°ƒç”¨ =========
if __name__ == "__main__":
    user_input1 = "åˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"
    user_input2 = "å†ç»™æˆ‘çœ‹çœ‹å…¶ä¸­æœ€å¤§çš„æ–‡ä»¶"

    print("ğŸ§  è¾“å…¥1:", user_input1)
    print("ğŸ’¬ è¾“å‡º1:", get_command_from_llm(user_input1, system_type="Linux", session_id="test2"))

    print("\nğŸ§  è¾“å…¥2:", user_input2)
    print("ğŸ’¬ è¾“å‡º2:", get_command_from_llm(user_input2, system_type="Linux", session_id="test2"))
