#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
llm_agent.py
è¨€é“ OS â€” è‡ªç„¶è¯­è¨€å‘½ä»¤è§£é‡Šæ¨¡å—ï¼ˆåŒå¡ç‰ˆï¼‰
"""
import os
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from utils.prompt_loader import load_system_prompt

# è®¾ç½®å¯è§ GPUï¼ˆ0 å’Œ 1ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

MODEL_PATH = "/data/SharedFile/Qwen/Qwen3-8B"

print(f"ğŸš€ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹ï¼š{MODEL_PATH}")

device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="balanced",
    trust_remote_code=True
)
model.eval()

messages = []

def init_vllm_prompt(system_type: str = None):
    """åŠ è½½ç³»ç»Ÿæç¤ºè¯"""
    SYSTEM_PROMPT = load_system_prompt(system_type)
    return [{"role": "system", "content": SYSTEM_PROMPT}]

def get_command_from_llm(prompt: str, system_type: str = None) -> str:
    """è°ƒç”¨æœ¬åœ°æ¨¡å‹ï¼Œæ ¹æ®è‡ªç„¶è¯­è¨€è¿”å›è§£é‡Š + å‘½ä»¤"""
    global messages
    if not messages:
        messages = init_vllm_prompt(system_type)

    messages.append({"role": "user", "content": prompt})
    text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.6,
            top_p=0.9,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    reply = re.sub(r"^.*?assistant", "", result, flags=re.DOTALL)
    reply = re.sub(r"<think>.*?</think>", "", reply, flags=re.DOTALL).strip()

    return reply
