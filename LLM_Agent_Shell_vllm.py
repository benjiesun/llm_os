# -*- coding: utf-8 -*-
"""
LLM_Agent_Shell_vllm.py
- æ”¯æŒ PROVIDER=local / api ä¸€é”®åˆ‡æ¢
- OpenAI Chat Completions å…¼å®¹åœ¨çº¿æ¥å£
- å‘½ä»¤ä¸¥æ ¼ç™½åå• + æ§åˆ¶ç¬¦/å±é™©è¯é˜»æ–­
- å­è¿›ç¨‹è¶…æ—¶ä¸è¾“å‡ºå¤§å°é™åˆ¶
- å¯¹è¯ä¸Šä¸‹æ–‡è£å‰ªï¼Œå›å¤è§£ææ›´ç¨³
- æ‰§è¡Œè§£é‡Šä½¿ç”¨ä¸´æ—¶ä¸Šä¸‹æ–‡ï¼Œé¿å…æ±¡æŸ“ä¸»ä¼šè¯

ä½¿ç”¨å‰ï¼š
1) å¦‚èµ°åœ¨çº¿APIï¼Œè¯·åœ¨åŒç›®å½•åˆ›å»º .envï¼ˆï¼š
   PROVIDER=api
   API_BASE=https://your-endpoint.example.com/v1
   API_KEY=sk-xxxxxxxxxxxxxxxx
   API_MODEL=gpt-4o-mini  # æˆ– qwen2.5-7b-instruct / llama-3.1-70b-instruct ç­‰
   API_TIMEOUT=30

2) å¦‚èµ°æœ¬åœ°æ¨¡å‹ï¼ˆé»˜è®¤ PROVIDER=localï¼‰ï¼Œè¯·æ”¹ MODEL_PATH ä¸ºä½ çš„æƒé‡è·¯å¾„ã€‚
"""

import os
import re
import json
import time
import shlex
import subprocess
from dataclasses import dataclass
from typing import List, Dict, Any

# ========== å¯é€‰ï¼šè¯»å– .env ==========
try:
    from dotenv import load_dotenv  # pip install python-dotenv
    load_dotenv()
except Exception:
    pass

# ========== æä¾›å•†é€‰æ‹© ==========
PROVIDER = os.getenv("PROVIDER", "local").lower()

# ========== æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆä»…å½“ PROVIDER=localï¼‰ ==========
MODEL_PATH = os.getenv("MODEL_PATH", "/home/Users/SharedFile/Models/Qwen/Qwen3-8B/")
USE_BF16_ON_CUDA = True

# ========== åœ¨çº¿ API é…ç½®ï¼ˆä»…å½“ PROVIDER=apiï¼‰ ==========
API_BASE = os.getenv("API_BASE", "").rstrip("/")
API_KEY = os.getenv("API_KEY", "")
API_MODEL = os.getenv("API_MODEL", "gpt-4o-mini")
API_TIMEOUT = float(os.getenv("API_TIMEOUT", "30"))

# ========== å›å¤è§£æ ==========
ASSISTANT_BLOCK_RE = re.compile(r"(?:<\|assistant\|>|\nassistant:)\s*(.*)", re.DOTALL)
def extract_assistant_text(decoded: str) -> str:
    m = ASSISTANT_BLOCK_RE.search(decoded)
    text = m.group(1) if m else decoded
    # å»æ‰ <think>...</think>
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
    return text

# ========== ç³»ç»Ÿæç¤ºï¼ˆå¼ºçº¦æŸåè®®ï¼‰ ==========
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ Shell ä»£ç†ã€‚å½“ä¸”ä»…å½“éœ€è¦æ‰§è¡Œç³»ç»Ÿå‘½ä»¤æ—¶ï¼Œè¾“å‡ºå¦‚ä¸‹æ ¼å¼ï¼ˆä¸è¦è¾“å‡ºå…¶ä»–å¤šä½™å­—ç¬¦ï¼‰ï¼š
EXECUTE:
<å•è¡Œå‘½ä»¤>

è§„åˆ™ï¼š
1) å¦‚æœä¸éœ€è¦æ‰§è¡Œå‘½ä»¤ï¼Œç›´æ¥ç”¨è‡ªç„¶è¯­è¨€å›ç­”ï¼Œä¸è¦å‡ºç° EXECUTE:ã€‚
2) å¦‚æœéœ€è¦æ‰§è¡Œå‘½ä»¤ï¼Œ<å•è¡Œå‘½ä»¤> å¿…é¡»æ˜¯å•è¡Œï¼Œä¸”ç¦æ­¢åŒ…å«ï¼šåˆ†å· ;ã€ç®¡é“ |ã€ä¸/æˆ– && ||ã€åå¼•å· `...`ã€$()ã€é‡å®šå‘ > >> < << | teeã€æ¢è¡Œã€å¤šæ¡å‘½ä»¤ç­‰æ§åˆ¶ç¬¦ã€‚
3) åšå†³é¿å…ä¸€åˆ‡å±é™©æ“ä½œï¼ˆåˆ é™¤/æ ¼å¼åŒ–/å…³æœº/é‡å¯/æƒé™/æŒ‚è½½/ç½‘ç»œç ´åç­‰ï¼‰ã€‚
4) å°½é‡é€‰æ‹©åªè¯»ã€ä¿¡æ¯æŸ¥è¯¢ç±»å‘½ä»¤ï¼Œä¾‹å¦‚ï¼šls, head, tail, grep(åªè¯»è·¯å¾„), wc, df, free, uname, whoami, pwd, du -sh <safe>, nvidia-smi ç­‰ã€‚
"""

# ========== æ¶ˆæ¯ä¸ä¸Šä¸‹æ–‡è£å‰ª ==========
MAX_TURNS = 8  # æœ€è¿‘ä¿ç•™ 8 è½® user/assistant
def trim_messages(messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
    if not messages:
        return messages
    sys = messages[0:1]
    tail = messages[1:]
    if len(tail) > 2 * MAX_TURNS:
        tail = tail[-2 * MAX_TURNS:]
    return sys + tail

# ========== å‘½ä»¤å®‰å…¨æ ¡éªŒï¼ˆç™½åå• + é˜»æ–­ï¼‰ ==========
ALLOWED_CMDS = {
    "ls","cat","head","tail","wc","df","free","nvidia-smi",
    "uname","whoami","pwd","du","grep","find"
}
FORBIDDEN_TOKENS = {";", "|", "||", "&&", "`", "$(", ")", ">", ">>", "<", "<<", "&", "\n", "\r", "\t"}
FORBIDDEN_WORDS  = {"sudo","rm","reboot","shutdown","mkfs","dd","kill","pkill","killall","mount","umount","chmod","chown","curl","nohup"}

# å¯é€‰ï¼šé™åˆ¶åªè¯»è·¯å¾„æ ¹ï¼ˆé˜²æ­¢éå†å…¨ç›˜ï¼‰ã€‚ä¸ºç©ºè¡¨ç¤ºä¸å¯ç”¨ã€‚
READONLY_ROOT = os.getenv("READONLY_ROOT", "").rstrip("/")

def is_under_readonly_root(path: str) -> bool:
    if not READONLY_ROOT:
        return True  # æœªå¯ç”¨é™åˆ¶
    try:
        full = os.path.realpath(path)
        base = os.path.realpath(READONLY_ROOT)
        return full.startswith(base + os.sep) or (full == base)
    except Exception:
        return False

def is_safe_command(cmd: str) -> bool:
    # æ§åˆ¶ç¬¦é˜»æ–­
    if any(t in cmd for t in FORBIDDEN_TOKENS):
        return False
    # æ‹†è¯æ£€æŸ¥
    try:
        parts = shlex.split(cmd)
    except Exception:
        return False
    if not parts:
        return False
    # ç™½åå•å‘½ä»¤
    if parts[0] not in ALLOWED_CMDS:
        return False
    low = cmd.lower()
    if any(w in low for w in FORBIDDEN_WORDS):
        return False
    # ç®€å•åªè¯»è·¯å¾„é™åˆ¶ï¼ˆå¯¹ cat/grep/find/du ç­‰å¸¸è§å¸¦è·¯å¾„çš„å‘½ä»¤çº¦æŸï¼‰
    def check_paths(args: List[str]) -> bool:
        paths = []
        skip_next = False
        for a in args:
            if skip_next:
                skip_next = False
                continue
            if a in {"-r","-R","-h","-s","-l","-a","-n","-m","-i","-v","-k","-H"}:
                continue
            if a in {"-C","-d","-p"}:  # æŸäº›å‘½ä»¤å‚æ•°å¸¦å€¼
                skip_next = True
                continue
            if a.startswith("-"):
                continue
            paths.append(a)
        for p in paths:
            # å¿½ç•¥æ˜æ˜¾éè·¯å¾„ï¼ˆæ¯”å¦‚çº¯å…³é”®å­—ï¼‰
            if p in {".",".."}:
                continue
            if not is_under_readonly_root(p):
                return False
        return True

    if parts[0] in {"cat","grep","find","du"}:
        if not check_paths(parts[1:]):
            return False
    return True

# ========== å­è¿›ç¨‹æ‰§è¡Œï¼ˆè¶…æ—¶ + è¾“å‡ºé™æµï¼‰ ==========
def execute_shell_command(command: str, timeout: int = 15, max_bytes: int = 1_000_000) -> str:
    try:
        result = subprocess.run(
            command, shell=True,
            capture_output=True, text=False,
            timeout=timeout,
        )
        out = (result.stdout or b"")[:max_bytes]
        err = (result.stderr or b"")[:max_bytes]
        text_out = out.decode(errors="replace").strip()
        text_err = err.decode(errors="replace").strip()
        if result.returncode == 0:
            return text_out if text_out else "(no output)"
        else:
            return f"âŒ é”™è¯¯ï¼š{text_err or '(empty stderr)'}"
    except subprocess.TimeoutExpired:
        return "â° å‘½ä»¤æ‰§è¡Œè¶…æ—¶ï¼ˆå·²ä¸­æ­¢ï¼‰"
    except Exception as e:
        return f"âš ï¸ æ‰§è¡Œå¼‚å¸¸ï¼š{e}"

# ========== LLM Provider æŠ½è±¡ ==========
class BaseProvider:
    def chat(self, messages: List[Dict[str,str]], max_new_tokens: int = 512, temperature: float = 0.7) -> str:
        raise NotImplementedError

# ---- åœ¨çº¿ API æä¾›å•†ï¼ˆOpenAI å…¼å®¹ï¼‰ ----
class OpenAICompatAPIProvider(BaseProvider):
    def __init__(self, api_base: str, api_key: str, model: str, timeout: float):
        assert api_base and api_key, "API_BASE æˆ– API_KEY æœªé…ç½®"
        self.base = api_base.rstrip("/")
        self.key = api_key
        self.model = model
        self.timeout = timeout

    def chat(self, messages: List[Dict[str,str]], max_new_tokens: int = 512, temperature: float = 0.7) -> str:
        import requests
        url = f"{self.base}/chat/completions"
        headers = {"Authorization": f"Bearer {self.key}", "Content-Type": "application/json"}
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": max(0.0, float(temperature)),
            "max_tokens": int(max_new_tokens),
        }
        for attempt in range(3):
            try:
                r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=self.timeout)
                if r.status_code == 200:
                    data = r.json()
                    return data["choices"][0]["message"]["content"]
                err = r.text[:500]
                if 400 <= r.status_code < 500:
                    return f"âŒ API é”™è¯¯ {r.status_code}: {err}"
                time.sleep(1.5 * (attempt + 1))
            except Exception as e:
                if attempt == 2:
                    return f"âš ï¸ API è¯·æ±‚å¼‚å¸¸ï¼š{e}"
                time.sleep(1.5 * (attempt + 1))
        return "âš ï¸ API æœªè¿”å›æœ‰æ•ˆç»“æœ"

# ---- æœ¬åœ° HF æ¨¡å‹æä¾›å•† ----
class LocalHFProvider(BaseProvider):
    def __init__(self, model_path: str):
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        self.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        self.DTYPE = torch.bfloat16 if (self.DEVICE == "cuda" and USE_BF16_ON_CUDA) else torch.float32
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, torch_dtype=self.DTYPE, trust_remote_code=True
        ).to(self.DEVICE).eval()

    def chat(self, messages: List[Dict[str,str]], max_new_tokens: int = 512, temperature: float = 0.7) -> str:
        import torch
        with torch.no_grad():
            text = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True, enable_thinking=False
            )
            inputs = self.tokenizer(text, return_tensors="pt").to(self.DEVICE)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=(temperature > 0.0),
                temperature=float(temperature) if temperature > 0.0 else 0.0,
                pad_token_id=self.tokenizer.eos_token_id,
            )
            decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return extract_assistant_text(decoded)

# ========== Provider å®ä¾‹ ==========
if PROVIDER == "api":
    provider: BaseProvider = OpenAICompatAPIProvider(API_BASE, API_KEY, API_MODEL, API_TIMEOUT)
else:
    provider: BaseProvider = LocalHFProvider(MODEL_PATH)

# ========== ç»Ÿä¸€ LLM è°ƒç”¨ ==========
def llm_chat(messages: List[Dict[str,str]], exec_mode: bool = False) -> str:
    # æ‰§è¡Œå‘½ä»¤åˆ¤åˆ«æ›´ç¨³å®šï¼štemperature=0, tokenså°äº›
    if exec_mode:
        return provider.chat(messages, max_new_tokens=200, temperature=0.0)
    # æ™®é€šèŠå¤©/è§£é‡Šï¼šä¿ç•™ä¸€å®šå¤šæ ·æ€§
    return provider.chat(messages, max_new_tokens=700, temperature=0.7)

# ========== è§£é‡Šè¾“å‡ºï¼šä¸´æ—¶ä¸Šä¸‹æ–‡ ==========
def explain_output(cmd: str, output: str) -> str:
    local_msgs = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„è¿ç»´è§£é‡ŠåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": f"å‘½ä»¤: {cmd}\nè¾“å‡º:\n{output[:5000]}\nè¯·ç”¨ç®€æ´è‡ªç„¶è¯­è¨€è¯´æ˜å…³é”®ç‚¹ã€‚"}
    ]
    return llm_chat(local_msgs, exec_mode=False)

# ========== ä¸»å¾ªç¯ ==========
def main():
    messages: List[Dict[str,str]] = [
        {"role": "system", "content": SYSTEM_PROMPT.strip()}
    ]
    print("âœ¨ Shell æ™ºèƒ½ä»£ç†å·²å¯åŠ¨ï¼ˆè¾“å…¥ exit/quit é€€å‡ºï¼‰ã€‚")
    if PROVIDER == "api":
        print(f"ğŸ”— æ¨¡å¼ï¼šåœ¨çº¿ API  -> {API_BASE}  æ¨¡å‹ï¼š{API_MODEL}")
    else:
        print(f"ğŸ§© æ¨¡å¼ï¼šæœ¬åœ°æ¨¡å‹  -> {MODEL_PATH}")

    while True:
        try:
            user_input = input("\nä½ ï¼š").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ å·²é€€å‡ºã€‚")
            break

        if user_input.lower() in {"exit","quit"}:
            print("ğŸ‘‹ å†è§ï½")
            break
        if not user_input:
            continue

        # ç»„è£…æ¶ˆæ¯å¹¶è£å‰ª
        messages.append({"role": "user", "content": user_input})
        messages[:] = trim_messages(messages)

        # ç¬¬ä¸€æ¬¡ç”Ÿæˆï¼šæ‰§è¡Œåˆ¤åˆ«ï¼ˆç¡®å®šæ€§ï¼‰
        reply = llm_chat(messages, exec_mode=True).strip()
        messages.append({"role": "assistant", "content": reply})
        messages[:] = trim_messages(messages)

        if reply.startswith("EXECUTE:"):
            cmd = reply.split("EXECUTE:", 1)[1].strip()
            print(f"ğŸ¤– å»ºè®®æ‰§è¡Œå‘½ä»¤: {cmd}")

            # å®‰å…¨æ ¡éªŒ
            if not is_safe_command(cmd):
                print("â›” å‘½ä»¤æœªé€šè¿‡å®‰å…¨æ ¡éªŒï¼Œå·²æ‹’ç»æ‰§è¡Œã€‚")
                continue

            confirm = input("æ˜¯å¦æ‰§è¡Œï¼Ÿ(y/n): ").strip().lower()
            if confirm != "y":
                print("âœ… å·²å–æ¶ˆæ‰§è¡Œã€‚")
                continue

            # æ‰§è¡Œ
            result = execute_shell_command(cmd)
            print("ğŸ“„ å‘½ä»¤è¾“å‡ºï¼š\n", result)

            # è§£é‡Šï¼ˆä¸´æ—¶ä¸Šä¸‹æ–‡ï¼Œä¸æ±¡æŸ“ä¸»å¯¹è¯ï¼‰
            explanation = explain_output(cmd, result)
            print(f"ğŸ“ è¾“å‡ºè§£é‡Šï¼š{explanation}")

        else:
            print(f"ğŸ¤–ï¼š{reply}")

if __name__ == "__main__":
    main()
