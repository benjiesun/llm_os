#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
llm_agent.py
è¨€é“ OS â€” è‡ªç„¶è¯­è¨€å‘½ä»¤è§£é‡Šæ¨¡å—ï¼ˆåŒå¡ç‰ˆï¼‰
"""
import platform
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

# âœ… è®¾ç½®å¯è§ GPUï¼ˆ0 å’Œ 1ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

MODEL_PATH = "/data/SharedFile/deepseek/DeepSeek-R1-Distill-Qwen-32B"

print(f"ğŸš€ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹ï¼š{MODEL_PATH}")

# è‡ªåŠ¨æ£€æµ‹ CUDA
device = "cuda" if torch.cuda.is_available() else "cpu"

# åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="balanced",   # âœ… è®©æ¨¡å‹è‡ªåŠ¨å¹³è¡¡åˆ†å¸ƒåˆ°ä¸¤å¼  GPU
    trust_remote_code=True
)

model.eval()

# ğŸ”¹ å¼ºåŒ– Prompt è§„èŒƒï¼Œè®©æ¨¡å‹è¾“å‡ºæ›´å¯æ§
# æ£€æµ‹å½“å‰ç³»ç»Ÿ
SYSTEM = platform.system()
# æ ¹æ®ç³»ç»ŸåŠ¨æ€è°ƒæ•´æç¤º
if SYSTEM == "Windows":
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€æ“ä½œç³»ç»ŸåŠ©æ‰‹ï¼Œå½“å‰è¿è¡Œåœ¨ Windows ç³»ç»Ÿã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€ï¼›
2. è¾“å‡ºä½ è¦æ‰§è¡Œçš„ä»»åŠ¡è¯´æ˜ï¼›
3. æœ€åç»™å‡ºå¯ä»¥ç›´æ¥æ‰§è¡Œçš„ Windows CMD å‘½ä»¤ã€‚

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
æˆ‘å°†ä¸ºä½ åšï¼š<ç®€çŸ­ä»»åŠ¡è¯´æ˜>ã€‚
å¯¹åº”çš„å‘½ä»¤æ˜¯ï¼š
<å‘½ä»¤>

ä¸è¦è¾“å‡ºå¤šä½™çš„è§£é‡Šã€ä¸Šä¸‹æ–‡æˆ–ä»£ç ã€‚
"""
else:
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€æ“ä½œç³»ç»ŸåŠ©æ‰‹ï¼Œå½“å‰è¿è¡Œåœ¨ Linux ç³»ç»Ÿã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€ï¼›
2. è¾“å‡ºä½ è¦æ‰§è¡Œçš„ä»»åŠ¡è¯´æ˜ï¼›
3. æœ€åç»™å‡ºå¯ä»¥ç›´æ¥æ‰§è¡Œçš„ Linux å‘½ä»¤ã€‚

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
æˆ‘å°†ä¸ºä½ åšï¼š<ç®€çŸ­ä»»åŠ¡è¯´æ˜>ã€‚
å¯¹åº”çš„å‘½ä»¤æ˜¯ï¼š
<å‘½ä»¤>

ä¸è¦è¾“å‡ºå¤šä½™çš„è§£é‡Šã€ä¸Šä¸‹æ–‡æˆ–ä»£ç ã€‚
"""

def get_command_from_llm(prompt: str) -> str:
    """
    è°ƒç”¨æœ¬åœ° DeepSeek æ¨¡å‹ï¼Œæ ¹æ®è‡ªç„¶è¯­è¨€è¿”å›è§£é‡Š + å‘½ä»¤ã€‚
    """
    full_prompt = f"{SYSTEM_PROMPT}\nç”¨æˆ·ï¼š{prompt}\nåŠ©æ‰‹ï¼š"
    inputs = tokenizer(full_prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.6,
            top_p=0.9,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
        )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # æå–åŠ©æ‰‹å›ç­”éƒ¨åˆ†
    if "åŠ©æ‰‹ï¼š" in result:
        result = result.split("åŠ©æ‰‹ï¼š", 1)[-1].strip()
    return result
