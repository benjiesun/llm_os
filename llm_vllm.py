#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
llm_vllm.py
è¨€é“ OS â€” è‡ªç„¶è¯­è¨€å‘½ä»¤è§£é‡Šæ¨¡å—ï¼ˆåŒå¡ç‰ˆï¼‰
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
from utils.prompt_loader import load_system_prompt


# âœ… è®¾ç½®å¯è§ GPUï¼ˆ0 å’Œ 1ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

MODEL_PATH = "/data/SharedFile/deepseek/DeepSeek-R1-Distill-Qwen-32B"

print(f"ğŸš€ æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹ï¼š{MODEL_PATH}")

# è‡ªåŠ¨æ£€æµ‹ CUDA
device = "cuda" if torch.cuda.is_available() else "cpu"

# åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="balanced",   # âœ… è®©æ¨¡å‹è‡ªåŠ¨å¹³è¡¡åˆ†å¸ƒåˆ°ä¸¤å¼  GPU
    trust_remote_code=True
)

model.eval()

def get_command_from_llm(prompt: str,system_type: str = None,) -> str:
    """
    è°ƒç”¨æœ¬åœ° DeepSeek æ¨¡å‹ï¼Œæ ¹æ®è‡ªç„¶è¯­è¨€è¿”å›è§£é‡Š + å‘½ä»¤ã€‚
    """

    # æ ¹æ®ç³»ç»Ÿç±»å‹åŠ è½½prompt
    SYSTEM_PROMPT = load_system_prompt(system_type)

    full_prompt = f"{SYSTEM_PROMPT}\nç”¨æˆ·ï¼š{prompt}\nåŠ©æ‰‹ï¼š"
    inputs = tokenizer(full_prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.6,
            top_p=0.9,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
        )

    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # æå–åŠ©æ‰‹å›ç­”éƒ¨åˆ†
    if "åŠ©æ‰‹ï¼š" in result:
        result = result.split("åŠ©æ‰‹ï¼š", 1)[-1].strip()
    return result
