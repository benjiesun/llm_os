#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
llm_api.py
å…¼å®¹ OpenAI / vLLM / å…¶ä»–èšåˆæœåŠ¡çš„åœ¨çº¿è°ƒç”¨æ¨¡å¼
"""
import requests
import os
from dotenv import load_dotenv
from utils.prompt_loader import load_system_prompt


# ========= é…ç½®åŒºåŸŸ =========
load_dotenv()
API_BASE = os.getenv("API_BASE", "https://api.deepseek.com/v1")          # æˆ– "http://127.0.0.1:8000/v1"
API_KEY  = os.getenv("API_KEY", "")                                     # åœ¨æœ¬åœ°ç”¨ .env æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡è®¾ç½®
API_MODEL = os.getenv("API_MODEL", "deepseek-chat")                    # æ¨¡å‹åç§°ï¼Œå¦‚ "deepseek-chat"
API_TIMEOUT = 60                                                     # è¯·æ±‚è¶…æ—¶ç§’æ•°



# ========= æ ¸å¿ƒå‡½æ•° =========
def get_command_from_api(prompt: str,system_type: str = None, max_new_tokens=512, temperature=0.7) -> str:
    """
    è°ƒç”¨å…¼å®¹ OpenAI Chat Completions API çš„è¿œç¨‹æ¨¡å‹ã€‚
    """
    SYSTEM_PROMPT = load_system_prompt(system_type)

    url = f"{API_BASE}/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}",
    }

    payload = {
        "model": API_MODEL,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        "temperature": temperature,
        "max_tokens": max_new_tokens,
        "stream": False
    }

    try:
        response = requests.post(url, headers=headers, json=payload, timeout=API_TIMEOUT)
        response.raise_for_status()
        data = response.json()
        return data["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"âŒ API è¯·æ±‚å¤±è´¥: {e}"

# ========= æµ‹è¯•è°ƒç”¨ =========
if __name__ == "__main__":
    user_input = "æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"
    print("ğŸ§  è¾“å…¥ï¼š", user_input)
    print("ğŸ’¬ è¾“å‡ºï¼š")
    print(get_command_from_api(user_input))
