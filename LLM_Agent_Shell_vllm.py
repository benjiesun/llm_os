import re
import subprocess
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ========== æ¨¡å‹é…ç½® ==========
MODEL_PATH = "/home/Users/SharedFile/Models/Qwen/Qwen3-8B/"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
).to(DEVICE).eval()

# ========== ç³»ç»Ÿæç¤º ==========
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½Shellä»£ç†(LLM Agent Shell)ã€‚
ä½ å…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
1. ç†è§£ç”¨æˆ·è‡ªç„¶è¯­è¨€è¯·æ±‚ï¼›   
2. åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡ŒLinuxå‘½ä»¤ï¼›
3. å½“éœ€è¦æ‰§è¡Œæ—¶ï¼Œç›´æ¥è¾“å‡ºå‘½ä»¤ï¼›
4. å½“ç”¨æˆ·åªæ˜¯æé—®æ—¶ï¼Œç›´æ¥å›ç­”ï¼›
5. é¿å…æ‰§è¡Œå±é™©å‘½ä»¤ï¼ˆå¦‚ rm, reboot, shutdown, :(){ ç­‰ï¼‰ï¼›
6. å½“ç”¨æˆ·æ‰§è¡Œå‘½ä»¤åï¼Œä½ èƒ½æ ¹æ®å‘½ä»¤è¾“å‡ºè¿›è¡Œæ€»ç»“æˆ–è§£é‡Šã€‚

è¾“å‡ºæ ¼å¼è§„åˆ™ï¼š
- å¦‚æœå†³å®šæ‰§è¡Œå‘½ä»¤ï¼Œè¯·è¾“å‡ºï¼š
EXECUTE:
<å‘½ä»¤>
- å¦‚æœä¸éœ€è¦æ‰§è¡Œå‘½ä»¤ï¼Œè¯·è¾“å‡ºè‡ªç„¶è¯­è¨€å›ç­”ã€‚
ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§å†…å®¹æˆ–æ€è€ƒè¿‡ç¨‹ã€‚
"""

messages = [{"role": "system", "content": SYSTEM_PROMPT}]

# ========== æ‰§è¡ŒShellå‘½ä»¤ ==========
def execute_shell_command(command):
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            return f"âŒ é”™è¯¯ï¼š{result.stderr.strip()}"
    except Exception as e:
        return f"âš ï¸ æ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}"

# ========== ä¸æ¨¡å‹äº¤äº’ ==========
def agent_chat(user_input):
    messages.append({"role": "user", "content": user_input})
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)
    inputs = tokenizer(text, return_tensors="pt").to(DEVICE)
    outputs = model.generate(
        **inputs,
        max_new_tokens=700,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )
    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "assistant" in reply:
        reply = reply.split("assistant")[-1]
    reply = re.sub(r"<think>.*?</think>", "", reply, flags=re.DOTALL).strip()
    messages.append({"role": "assistant", "content": reply})
    return reply

# ========== è§£é‡Šå‘½ä»¤è¾“å‡º ==========
def explain_output(cmd, output):
    prompt = f"å‘½ä»¤: {cmd}\nè¾“å‡º:\n{output}\nè¯·ç”¨ç®€æ´è‡ªç„¶è¯­è¨€è§£é‡Šè¿™ä¸ªè¾“å‡ºã€‚"
    messages.append({"role": "user", "content": prompt})

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)
    inputs = tokenizer(text, return_tensors="pt").to(DEVICE)
    outputs = model.generate(
        **inputs,
        max_new_tokens=500,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
    )
    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "assistant" in reply:
        reply = reply.split("assistant")[-1]
    reply = re.sub(r"<think>.*?</think>", "", reply, flags=re.DOTALL).strip()
    messages.append({"role": "assistant", "content": reply})
    return reply

# ========== ä¸»äº¤äº’ ==========
def main():
    print("ğŸ¤– æ¬¢è¿ä½¿ç”¨ LLM Agent Shellï¼è¾“å…¥ exit é€€å‡ºã€‚\n")

    while True:
        user_input = input("ğŸ§‘ ä½ ï¼š")
        if user_input.lower() in ["exit", "quit"]:
            print("ğŸ¤–ï¼šå†è§ğŸ‘‹")
            break

        reply = agent_chat(user_input)

        if "EXECUTE:" in reply:
            cmd = reply.split("EXECUTE:")[1].strip()

            # å±é™©å‘½ä»¤æ£€æµ‹
            if any(danger in cmd for danger in ["rm", "shutdown", "reboot", ":(){", "mkfs", "dd", "kill", ">:"]):
                print("âš ï¸ æ£€æµ‹åˆ°å±é™©å‘½ä»¤ï¼Œå·²æ‹’ç»æ‰§è¡Œã€‚")
                continue

            print(f"ğŸ¤– å»ºè®®æ‰§è¡Œå‘½ä»¤: {cmd}")
            confirm = input("æ˜¯å¦æ‰§è¡Œï¼Ÿ(y/n): ")
            if confirm.lower() == "y":
                result = execute_shell_command(cmd)
                print("å‘½ä»¤è¾“å‡ºï¼š\n", result)
                explanation = explain_output(cmd, result)
                print(f"ğŸ¤– è¾“å‡ºå†…å®¹è§£é‡Šï¼š{explanation}")
            else:
                print("âœ… å·²å–æ¶ˆæ‰§è¡Œã€‚")
        else:
            print(f"ğŸ¤–ï¼š{reply}")

if __name__ == "__main__":
    main()
